{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5174e1a1-4e1c-4b3b-a9d3-a7f93a180db3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5174e1a1-4e1c-4b3b-a9d3-a7f93a180db3",
    "outputId": "f23a6ee9-b5c8-43ae-8dba-c8d22970fd11"
   },
   "outputs": [],
   "source": [
    "import openml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e144d-0b0d-41bc-b5ab-4ae90a7305c0",
   "metadata": {
    "id": "2f2e144d-0b0d-41bc-b5ab-4ae90a7305c0"
   },
   "source": [
    "# Import danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b368e2-6ec0-4482-892b-aec8d3857453",
   "metadata": {
    "id": "10b368e2-6ec0-4482-892b-aec8d3857453"
   },
   "outputs": [],
   "source": [
    "datasets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49b92a-15bd-45d4-bf47-0f2a3227eb1d",
   "metadata": {
    "id": "4a49b92a-15bd-45d4-bf47-0f2a3227eb1d"
   },
   "source": [
    "### Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1625e297-2c86-4c0b-b489-ef737191d390",
   "metadata": {
    "id": "1625e297-2c86-4c0b-b489-ef737191d390"
   },
   "outputs": [],
   "source": [
    "diabetes_dataset = openml.datasets.get_dataset(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4798ee-3f7d-4306-a14d-fd54a72098ff",
   "metadata": {
    "id": "4d4798ee-3f7d-4306-a14d-fd54a72098ff"
   },
   "outputs": [],
   "source": [
    "X, y, _, columns = diabetes_dataset.get_data(target=diabetes_dataset.default_target_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "376f06f5-d8b9-4aa8-97b9-5eeba7c56df7",
   "metadata": {
    "id": "376f06f5-d8b9-4aa8-97b9-5eeba7c56df7"
   },
   "outputs": [],
   "source": [
    "datasets['diabetes'] = [X, y, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f3873f-d932-4cb3-b974-bd12a0956ab1",
   "metadata": {
    "id": "41f3873f-d932-4cb3-b974-bd12a0956ab1"
   },
   "source": [
    "### Credit-g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "291083db-deb2-4e88-91dd-724e6904a6b7",
   "metadata": {
    "id": "291083db-deb2-4e88-91dd-724e6904a6b7"
   },
   "outputs": [],
   "source": [
    "creditg_dataset = openml.datasets.get_dataset(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0ac34c-184e-4dfb-bb84-a50c989f6a58",
   "metadata": {
    "id": "1b0ac34c-184e-4dfb-bb84-a50c989f6a58"
   },
   "outputs": [],
   "source": [
    "X, y, _, columns = creditg_dataset.get_data(target=creditg_dataset.default_target_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ddf5885-d605-4a60-9826-a5db47815190",
   "metadata": {
    "id": "1ddf5885-d605-4a60-9826-a5db47815190"
   },
   "outputs": [],
   "source": [
    "datasets['creditg'] = [X, y, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f8190-05ee-4385-82c3-17fed2379bb1",
   "metadata": {
    "id": "1c2f8190-05ee-4385-82c3-17fed2379bb1"
   },
   "source": [
    "### Spambase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb226b6-824c-466d-9216-03d4b09e381f",
   "metadata": {
    "id": "7cb226b6-824c-466d-9216-03d4b09e381f"
   },
   "outputs": [],
   "source": [
    "spambase_dataset = openml.datasets.get_dataset(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54808c77-324c-4b3a-83a3-e6d6501cdf43",
   "metadata": {
    "id": "54808c77-324c-4b3a-83a3-e6d6501cdf43"
   },
   "outputs": [],
   "source": [
    "X, y, _, columns = spambase_dataset.get_data(target=spambase_dataset.default_target_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a3951db-efcd-48a1-bf59-d2bfb22c9ed8",
   "metadata": {
    "id": "2a3951db-efcd-48a1-bf59-d2bfb22c9ed8"
   },
   "outputs": [],
   "source": [
    "datasets['spambase'] = [X, y, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42ce24-ac2a-409b-b28a-0164b7406211",
   "metadata": {
    "id": "6b42ce24-ac2a-409b-b28a-0164b7406211"
   },
   "source": [
    "### Yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f2ba50-ce18-4f51-b8ac-5cb9c775c9e8",
   "metadata": {
    "id": "47f2ba50-ce18-4f51-b8ac-5cb9c775c9e8"
   },
   "outputs": [],
   "source": [
    "yeast_dataset = openml.datasets.get_dataset(40597)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fddaea73-9c39-4b98-b138-c6275fd4aa77",
   "metadata": {
    "id": "fddaea73-9c39-4b98-b138-c6275fd4aa77"
   },
   "outputs": [],
   "source": [
    "#X, y, _, columns = yeast_dataset.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c8ed080-78a7-4bf0-90ab-50d5f41a598e",
   "metadata": {
    "id": "8c8ed080-78a7-4bf0-90ab-50d5f41a598e"
   },
   "outputs": [],
   "source": [
    "#datasets['yeast'] = [X, y, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a74372-c376-48ea-b049-43686e7d7a38",
   "metadata": {
    "id": "b2a74372-c376-48ea-b049-43686e7d7a38"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "790217c5-1eb6-48d0-ad5b-dda0b31ef2f8",
   "metadata": {
    "id": "790217c5-1eb6-48d0-ad5b-dda0b31ef2f8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "Wm9UzPJw4P2o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wm9UzPJw4P2o",
    "outputId": "4ff0dd76-e804-4c1b-c92f-d402ea0a9180",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('preprocessing',\n",
       "   ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                     transformers=[('num_pipeline',\n",
       "                                    Pipeline(steps=[('impute', SimpleImputer()),\n",
       "                                                    ('scale', MinMaxScaler())]),\n",
       "                                    <sklearn.compose._column_transformer.make_column_selector object at 0x000002177FF759D0>),\n",
       "                                   ('cat_pipeline',\n",
       "                                    Pipeline(steps=[('impute',\n",
       "                                                     SimpleImputer(strategy='most_frequent')),\n",
       "                                                    ('one-hot',\n",
       "                                                     OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                   sparse_output=False))]),\n",
       "                                    <sklearn.compose._column_transformer.make_column_selector object at 0x000002177F9B7EC0>)])),\n",
       "  ('model',\n",
       "   LogisticRegression(class_weight='balanced', l1_ratio=0.5, penalty='elasticnet',\n",
       "                      solver='saga'))],\n",
       " 'verbose': False,\n",
       " 'preprocessing': ColumnTransformer(n_jobs=-1, remainder='passthrough',\n",
       "                   transformers=[('num_pipeline',\n",
       "                                  Pipeline(steps=[('impute', SimpleImputer()),\n",
       "                                                  ('scale', MinMaxScaler())]),\n",
       "                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000002177FF759D0>),\n",
       "                                 ('cat_pipeline',\n",
       "                                  Pipeline(steps=[('impute',\n",
       "                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                  ('one-hot',\n",
       "                                                   OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                 sparse_output=False))]),\n",
       "                                  <sklearn.compose._column_transformer.make_column_selector object at 0x000002177F9B7EC0>)]),\n",
       " 'model': LogisticRegression(class_weight='balanced', l1_ratio=0.5, penalty='elasticnet',\n",
       "                    solver='saga'),\n",
       " 'preprocessing__force_int_remainder_cols': True,\n",
       " 'preprocessing__n_jobs': -1,\n",
       " 'preprocessing__remainder': 'passthrough',\n",
       " 'preprocessing__sparse_threshold': 0.3,\n",
       " 'preprocessing__transformer_weights': None,\n",
       " 'preprocessing__transformers': [('num_pipeline',\n",
       "   Pipeline(steps=[('impute', SimpleImputer()), ('scale', MinMaxScaler())]),\n",
       "   <sklearn.compose._column_transformer.make_column_selector at 0x2177ff759d0>),\n",
       "  ('cat_pipeline',\n",
       "   Pipeline(steps=[('impute', SimpleImputer(strategy='most_frequent')),\n",
       "                   ('one-hot',\n",
       "                    OneHotEncoder(handle_unknown='ignore', sparse_output=False))]),\n",
       "   <sklearn.compose._column_transformer.make_column_selector at 0x2177f9b7ec0>)],\n",
       " 'preprocessing__verbose': False,\n",
       " 'preprocessing__verbose_feature_names_out': True,\n",
       " 'preprocessing__num_pipeline': Pipeline(steps=[('impute', SimpleImputer()), ('scale', MinMaxScaler())]),\n",
       " 'preprocessing__cat_pipeline': Pipeline(steps=[('impute', SimpleImputer(strategy='most_frequent')),\n",
       "                 ('one-hot',\n",
       "                  OneHotEncoder(handle_unknown='ignore', sparse_output=False))]),\n",
       " 'preprocessing__num_pipeline__memory': None,\n",
       " 'preprocessing__num_pipeline__steps': [('impute', SimpleImputer()),\n",
       "  ('scale', MinMaxScaler())],\n",
       " 'preprocessing__num_pipeline__verbose': False,\n",
       " 'preprocessing__num_pipeline__impute': SimpleImputer(),\n",
       " 'preprocessing__num_pipeline__scale': MinMaxScaler(),\n",
       " 'preprocessing__num_pipeline__impute__add_indicator': False,\n",
       " 'preprocessing__num_pipeline__impute__copy': True,\n",
       " 'preprocessing__num_pipeline__impute__fill_value': None,\n",
       " 'preprocessing__num_pipeline__impute__keep_empty_features': False,\n",
       " 'preprocessing__num_pipeline__impute__missing_values': nan,\n",
       " 'preprocessing__num_pipeline__impute__strategy': 'mean',\n",
       " 'preprocessing__num_pipeline__scale__clip': False,\n",
       " 'preprocessing__num_pipeline__scale__copy': True,\n",
       " 'preprocessing__num_pipeline__scale__feature_range': (0, 1),\n",
       " 'preprocessing__cat_pipeline__memory': None,\n",
       " 'preprocessing__cat_pipeline__steps': [('impute',\n",
       "   SimpleImputer(strategy='most_frequent')),\n",
       "  ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))],\n",
       " 'preprocessing__cat_pipeline__verbose': False,\n",
       " 'preprocessing__cat_pipeline__impute': SimpleImputer(strategy='most_frequent'),\n",
       " 'preprocessing__cat_pipeline__one-hot': OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
       " 'preprocessing__cat_pipeline__impute__add_indicator': False,\n",
       " 'preprocessing__cat_pipeline__impute__copy': True,\n",
       " 'preprocessing__cat_pipeline__impute__fill_value': None,\n",
       " 'preprocessing__cat_pipeline__impute__keep_empty_features': False,\n",
       " 'preprocessing__cat_pipeline__impute__missing_values': nan,\n",
       " 'preprocessing__cat_pipeline__impute__strategy': 'most_frequent',\n",
       " 'preprocessing__cat_pipeline__one-hot__categories': 'auto',\n",
       " 'preprocessing__cat_pipeline__one-hot__drop': None,\n",
       " 'preprocessing__cat_pipeline__one-hot__dtype': numpy.float64,\n",
       " 'preprocessing__cat_pipeline__one-hot__feature_name_combiner': 'concat',\n",
       " 'preprocessing__cat_pipeline__one-hot__handle_unknown': 'ignore',\n",
       " 'preprocessing__cat_pipeline__one-hot__max_categories': None,\n",
       " 'preprocessing__cat_pipeline__one-hot__min_frequency': None,\n",
       " 'preprocessing__cat_pipeline__one-hot__sparse_output': False,\n",
       " 'model__C': 1.0,\n",
       " 'model__class_weight': 'balanced',\n",
       " 'model__dual': False,\n",
       " 'model__fit_intercept': True,\n",
       " 'model__intercept_scaling': 1,\n",
       " 'model__l1_ratio': 0.5,\n",
       " 'model__max_iter': 100,\n",
       " 'model__multi_class': 'deprecated',\n",
       " 'model__n_jobs': None,\n",
       " 'model__penalty': 'elasticnet',\n",
       " 'model__random_state': None,\n",
       " 'model__solver': 'saga',\n",
       " 'model__tol': 0.0001,\n",
       " 'model__verbose': 0,\n",
       " 'model__warm_start': False}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting preprocessing and model\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale', MinMaxScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "col_trans = ColumnTransformer(transformers=[\n",
    "        ('num_pipeline', num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "        ('cat_pipeline', cat_pipeline, make_column_selector(dtype_include='category'))\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "def label_encode(y):\n",
    "    label_encoder = LabelEncoder()\n",
    "    return label_encoder.fit_transform(y)\n",
    "\n",
    "target_transformer = FunctionTransformer(label_encode, validate=False)\n",
    "\n",
    "LR = LogisticRegression(penalty='elasticnet', solver='saga', class_weight='balanced', l1_ratio=0.5)\n",
    "model_pipe = Pipeline([('preprocessing', col_trans), ('model', LR)])\n",
    "model_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c255aed-9168-4d27-ad18-b84a7bbaf963",
   "metadata": {},
   "source": [
    "# Szukanie hiperparametrów domyślnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c6d15-cece-4d3a-bb91-8b5fb7f58628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c36a1781-a4c4-4bf9-90fd-b14d01f22e9f",
   "metadata": {
    "id": "c36a1781-a4c4-4bf9-90fd-b14d01f22e9f"
   },
   "outputs": [],
   "source": [
    "# Definicja modelu i parametrów do przeszukiwania\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': [50, 100, 200, 300, 1000, 2000],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2f5b0a88-5eab-4d90-8660-0a0d6457bb03",
   "metadata": {
    "id": "2f5b0a88-5eab-4d90-8660-0a0d6457bb03"
   },
   "outputs": [],
   "source": [
    "# Przeprowadzenie Random Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "132fe609-1e96-4bfe-bb8d-2ca4f6251e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: diabetes\n",
      "Processing dataset: creditg\n",
      "Processing dataset: spambase\n"
     ]
    }
   ],
   "source": [
    "# Przechowujemy wyniki dla każdego zbioru danych\n",
    "all_results = []\n",
    "\n",
    "# Przeprowadzamy Random Search dla każdego zbioru danych\n",
    "for dataset_name in datasets:\n",
    "    start_time_iter = time()\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    \n",
    "    # Ekstrakcja danych\n",
    "    dataset = datasets[dataset_name]\n",
    "    X = dataset[0]\n",
    "    y = target_transformer.fit_transform(dataset[1])  # Target transformation\n",
    "    \n",
    "    # Transformacja cech\n",
    "    X = col_trans.fit_transform(X)\n",
    "    \n",
    "    # Podział na dane treningowe i testowe\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n",
    "    \n",
    "    # Dopasowanie modelu\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Zbieranie wyników każdej iteracji z Random Search dla bieżącego zbioru danych\n",
    "    dataset_results = pd.DataFrame(random_search.cv_results_)\n",
    "    dataset_results['dataset'] = dataset_name\n",
    "    \n",
    "    # Zamiana słownika z parametrami na tekstowy format JSON\n",
    "    dataset_results['params'] = dataset_results['params'].apply(lambda x: str(x))\n",
    "    \n",
    "    # Zapis wyników dla bieżącego zbioru danych do ogólnej listy\n",
    "    all_results.append(dataset_results[['params', 'mean_test_score', 'dataset']])\n",
    "\n",
    "    # Odliczanie czasu całego algorytmu\n",
    "        elapsed_time_iter = time() - start_time_iter\n",
    "        print(f\"Czas wykonaniu dla datasetu {dataset_name}: {elapsed_time_iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "253c800f-9f6c-45d1-af53-f47a61b7acd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "params      {'n_estimators': 300, 'min_samples_split': 10,...\n",
      "mean_auc                                             0.866087\n",
      "Name: 11, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Konwersja wyników do DataFrame\n",
    "all_results_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Grupowanie po parametrach i obliczenie średniej AUC\n",
    "mean_results_df = (\n",
    "    all_results_df.groupby('params')['mean_test_score']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'mean_test_score': 'mean_auc'})\n",
    ")\n",
    "\n",
    "# Znalezienie najlepszego zestawu hiperparametrów\n",
    "default_params = mean_results_df.loc[mean_results_df['mean_auc'].idxmax()]\n",
    "\n",
    "# Wyświetlanie najlepszego zestawu hiperparametrów i jego średniej wartości AUC\n",
    "print(\"Best Parameters:\")\n",
    "print(default_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8bd1c-3e51-4f66-883c-28d4b5202be4",
   "metadata": {
    "id": "dfd8bd1c-3e51-4f66-883c-28d4b5202be4"
   },
   "source": [
    "# Random Search Optymalizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a8f4bb6-c281-4367-987a-0adaae1e5c3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "0a8f4bb6-c281-4367-987a-0adaae1e5c3d",
    "outputId": "ff691a34-add4-4b2e-a357-e0a1a8b6179a"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Wyłączenie FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignorowanie wszystkich UserWarnings (np. z skopt) oraz ConvergenceWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "rN51d96z5w_t",
   "metadata": {
    "id": "rN51d96z5w_t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: diabetes\n",
      "Czas wykonaniu dla datasetu diabetes: 20.550113201141357\n",
      "Dataset: creditg\n",
      "Czas wykonaniu dla datasetu creditg: 19.454925537109375\n",
      "Dataset: spambase\n",
      "Czas wykonaniu dla datasetu spambase: 65.23516201972961\n"
     ]
    }
   ],
   "source": [
    "# Obliczanie dla każdego datasetu\n",
    "for i, dataset_name in enumerate(datasets):\n",
    "    start_time_iter = time()\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    # Eksportowanie datasetu\n",
    "    dataset = datasets[dataset_name]\n",
    "    X = dataset[0]\n",
    "    y = dataset[1]\n",
    "    columns = dataset[2]\n",
    "    \n",
    "    y = target_transformer.fit_transform(y)\n",
    "\n",
    "    X = col_trans.fit_transform(X)\n",
    "\n",
    "    # Konwersja do DataFrame, jeśli X nie jest DataFrame\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "    # Podział na dane treningowe i testowe\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n",
    "    \n",
    "    # Unoptimized\n",
    "    start_time = time()\n",
    "    model_pipe.fit(X_train,y_train)\n",
    "    elapsed_time = time() - start_time\n",
    "    \n",
    "    y_pred = model_pipe.predict(X_test)\n",
    "    auc_scr = roc_auc_score(y_test, y_pred)\n",
    "    r2_scr = r2_score(y_test, y_pred)\n",
    "    \n",
    "    new_row = pd.DataFrame({\n",
    "      'method': \"Unoptimized\",\n",
    "      'elapsed_time': [elapsed_time],\n",
    "      'best_score': None,\n",
    "      'test_score': None,\n",
    "      'best_params': f\"[\\'model__C\\': {LR.get_params()['C']}, \\'model__l1_ratio\\':{LR.get_params()['l1_ratio']}]\",\n",
    "      'auc_score': [auc_scr],\n",
    "      'r2_score': [r2_scr]\n",
    "    })\n",
    "    main_results_df = pd.DataFrame(columns=['method','elapsed_time', 'best_score', 'test_score', 'best_params', 'auc_score', 'r2_score'])\n",
    "    main_results_df = pd.concat([main_results_df, new_row], ignore_index=True)\n",
    "\n",
    "    # Random Search\n",
    "    start_time = time()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    elapsed_time = time() - start_time\n",
    "    \n",
    "    best_score = random_search.best_score_\n",
    "    test_score = random_search.score(X_test, y_test)\n",
    "    best_params = str(random_search.best_params_)\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    auc_scr = roc_auc_score(y_test, y_pred)\n",
    "    r2_scr = r2_score(y_test, y_pred)\n",
    "    \n",
    "    random_results = pd.DataFrame(random_search.cv_results_)\n",
    "    random_results['search_type'] = 'RandomizedSearchCV'\n",
    "    \n",
    "    new_row = pd.DataFrame({\n",
    "        'method': \"RandomSearchCV\",\n",
    "        'elapsed_time': [elapsed_time],\n",
    "        'best_score': [best_score],\n",
    "        'test_score': [test_score],\n",
    "        'best_params': [best_params],\n",
    "        'auc_score': [auc_scr],\n",
    "        'r2_score': [r2_scr]\n",
    "    })\n",
    "    main_results_df = pd.concat([main_results_df, new_row], ignore_index=True)\n",
    "\n",
    "    name = \"random_search\"\n",
    "    main_results_df.to_csv(f\"output/{dataset_name}_{name}_main_res.csv\", index=False)\n",
    "    random_results.to_csv(f\"output/{dataset_name}_{name}_random_iter_res.csv\", index=False)\n",
    "\n",
    "    # Odliczanie czasu całego algorytmu\n",
    "    elapsed_time_iter = time() - start_time_iter\n",
    "    print(f\"Czas wykonaniu dla datasetu {dataset_name}: {elapsed_time_iter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f237545-b9db-4aa5-858c-6495450f4bf1",
   "metadata": {
    "id": "1f237545-b9db-4aa5-858c-6495450f4bf1"
   },
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1fe3511-bb9a-4e56-9027-aedd84d7c44a",
   "metadata": {
    "id": "e1fe3511-bb9a-4e56-9027-aedd84d7c44a"
   },
   "outputs": [],
   "source": [
    "# Definicja przestrzeni hiperparametrów\n",
    "param_space = {\n",
    "    \"n_estimators\": Integer(50, 300),\n",
    "    #\"max_depth\": Integer(5, 20),\n",
    "    #\"min_samples_split\": Integer(2, 10),\n",
    "    #\"min_samples_leaf\": Integer(1, 4)\n",
    "    #,\"bootstrap\": Categorical([True, False])\n",
    "}\n",
    "\n",
    "param_space = {\n",
    "    'model__C': [0.1, 1, 10],                # Parametr C dla LogisticRegression\n",
    "    'model__penalty': ['elasticnet'],        # Parametr penalty\n",
    "    'model__l1_ratio': [0.2, 0.5, 0.8]       # Wartość l1_ratio dla elasticnet\n",
    "}\n",
    "\n",
    "bayes_search = BayesSearchCV(\n",
    "    model_pipe,\n",
    "    param_space,\n",
    "    n_iter=60,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=8,\n",
    "    return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b38493e4-0314-497e-a7a7-615dc8ad18af",
   "metadata": {
    "id": "b38493e4-0314-497e-a7a7-615dc8ad18af",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: diabetes\n",
      "Dataset: creditg\n",
      "Dataset: spambase\n"
     ]
    }
   ],
   "source": [
    "# Obliczanie dla każdego datasetu\n",
    "for i, dataset_name in enumerate(datasets):\n",
    "    start_time_iter = time()\n",
    "    \n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    # Eksportowanie datasetu\n",
    "    dataset = datasets[dataset_name]\n",
    "    X = dataset[0]\n",
    "    y = dataset[1]\n",
    "    columns = dataset[2]\n",
    "    \n",
    "    y = target_transformer.fit_transform(y)\n",
    "\n",
    "    X = col_trans.fit_transform(X)\n",
    "\n",
    "    # Konwersja do DataFrame, jeśli X nie jest DataFrame\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "    # Podział na dane treningowe i testowe\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n",
    "    \n",
    "    # Unoptimized\n",
    "    start_time = time()\n",
    "    model_pipe.fit(X_train,y_train)\n",
    "    elapsed_time = time() - start_time\n",
    "    \n",
    "    y_pred = model_pipe.predict(X_test)\n",
    "    auc_scr = roc_auc_score(y_test, y_pred)\n",
    "    r2_scr = r2_score(y_test, y_pred)\n",
    "    \n",
    "    new_row = pd.DataFrame({\n",
    "      'method': \"Unoptimized\",\n",
    "      'elapsed_time': [elapsed_time],\n",
    "      'best_score': None,\n",
    "      'test_score': None,\n",
    "      'best_params': f\"[\\'model__C\\': {LR.get_params()['C']}, \\'model__l1_ratio\\':{LR.get_params()['l1_ratio']}]\",\n",
    "      'auc_score': [auc_scr],\n",
    "      'r2_score': [r2_scr]\n",
    "    })\n",
    "    main_results_df = pd.DataFrame(columns=['method','elapsed_time', 'best_score', 'test_score', 'best_params', 'auc_score', 'r2_score'])\n",
    "    main_results_df = pd.concat([main_results_df, new_row], ignore_index=True)\n",
    "\n",
    "    # Bayes Search    \n",
    "    start_time = time()\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    elapsed_time = time() - start_time\n",
    "    \n",
    "    best_score = bayes_search.best_score_\n",
    "    test_score = bayes_search.score(X_test, y_test)\n",
    "    best_params = str(bayes_search.best_params_)\n",
    "    \n",
    "    best_model = bayes_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    auc_scr = roc_auc_score(y_test, y_pred)\n",
    "    r2_scr = r2_score(y_test, y_pred)\n",
    "    \n",
    "    bayes_results = pd.DataFrame(bayes_search.cv_results_)\n",
    "    bayes_results['search_type'] = 'BayesSearchCV'\n",
    "    \n",
    "    new_row = pd.DataFrame({\n",
    "        'method': \"BayesSearchCV\",\n",
    "        'elapsed_time': [elapsed_time],\n",
    "        'best_score': [best_score],\n",
    "        'test_score': [test_score],\n",
    "        'best_params': [best_params],\n",
    "        'auc_score': [auc_scr],\n",
    "        'r2_score': [r2_scr]\n",
    "    })\n",
    "    main_results_df = pd.concat([main_results_df, new_row], ignore_index=True)\n",
    "\n",
    "    name = \"bayes_search\"\n",
    "    main_results_df.to_csv(f\"output/{dataset_name}_{name}_main_res_{i}.csv\", index=False)\n",
    "    bayes_results.to_csv(f\"output/{dataset_name}_{name}_bayes_iter_res_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8ea46-2866-4939-bd6d-759ce3634aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
